 # Модификация AngelinaReader
 Здесь описана работа, сделанная в рамках курса "Обработка и интерптетация сигналов": что я делал, что получилось и как это воспроизвести.
 
 ## Первоначальные намерения
 
 ### Постановка задачи
 
 *Шрифт Брайля* - система тактильного письма для незрячих, где литеры обозначаются комбинациями выпуклых точек.
 В брайлевских книгах текст печатается с двух сторон (на обороте листа точки выглядят как ямочки, но под пальцем почти не чувствуются).
 При письме на *брайлевском приборе* (маленькое тупое шило - *стилус* + трафарет) точки только на одной стороне.
 
 Иногда возникает потребность перевести брайлевский текст в обычный (например, при инклюзивном образовании:
 незрячий ребёнок учится в школе с обычными детьми, но преподаватели не знают шрифт Брайля).
 В 2020 г. Илья Геннадиевич Оводов опубликовал программу распознавания брайлевского текста AngelinaReader на основе нейросети RetinaNet.
 
 Необходимо попытаться исправить или смягчить недостатки программы:
 1. Чувствительность к положению источника света
 1. Плохое качество распознавания при наличии перспективных искажений
 1. Плохое качество распознавания на не-бумажных носителях
 1. Чувствительность к повороту листа в горизонтальной плоскости
 
 ### Требования к входным данным
 
На входе цветной снимок со смартфона или планшета в портретной или альбомной ориентации в качестве не менее 1024x1024 пикселей (например, 1024x1376 или 2160x1840 пикселей).
На снимке изображение одного из следующих объектов:

1. Лист брайлевской книги с точками на лицевой и оборотной стороне (белая бумага A4, см. примеры)
1. Лист рукописи с символами Брайля на лицевой и оборотной стороне (также белая бумага A4 или A5)
1. одна из тех табличек, которые расположены на дверях кабинетов в Научно-исследовательском корпусе Политеха
1. обучающие брайлевские карточки с QR-кодами
 
 Подробнее про постановку и требования см. [февральскую дорожную карту](https://github.com/braille-systems/brl_ocr/wiki/roadmap-feb2021-ru).
 
 ### План решения
 
Предполагалось действовать так:

1. Изучить AngelinaReader и осуществить тренировку нейросети на публично доступных данных (DSBI, AngelinaDataset)
1. Попробовать различные аугментации данных, чтобы уменьшить проблемы с распознаванием повёрнутых и искажённых листов
1. Отснять 20 изображений брайлевского текста с различными искажениями (нынешние датасеты содержат довольно качественные образцы)
 и разметить их, после чего проверить, когда они лучше распознаются: с аугментациями или без.
1. Отснять ещё 150-300 изображений брайлевского текста и попробовать SSL (semi-supervised learning):
уже обученную нейросеть запустить на этих данных, после чего её ответ - полученные псевдо-метки - использовать на следующей итерации обучения.
1. Отснять 150-300 фотографий табличек Брайля и попробовать SSL с ними.
1. Сделать 10-20 размеченных фотографий табличек Брайля и сравнить качество их распознавания моделями, обученными различным образом.

Подробнее можно посмотреть в [тексте февральского плана](https://github.com/braille-systems/brl_ocr/wiki/plan-feb2021-ru).

 ## Ход реализации

Вначале я сделал в fork репозиториев [AngelinaReader](https://github.com/braille-systems/AngelinaReader), [OvoTools](https://github.com/braille-systems/OvoTools), [pytorch-retinanet](https://github.com/braille-systems/pytorch-retinanet), [DSBI](https://github.com/braille-systems/DSBI) и [AngelinaDataset](https://github.com/braille-systems/AngelinaDataset).
Чтобы корректно запустить тренировку AngelinaReader на моей Windows-машине с Python3.8, пришлось немного модифицировать что-то в каждом репозитории (за исключением AngelinaDataset).
Также составил в README пошаговую инструкцию по запуску, а ещё попытался настроить CI на GitHub Actions (статические проверки кода, подсчёт покрытия), но он пока падает (как минимум из-за того, что статический анализ не проходит) + подключил CodeFactor.
Кроме того, сделал автоформатирование кода утилитой `black`.
Сейчас эта версия AngelinaReader содержится в ветке `develop`.

Затем приступил к работе над планом и по ходу действий внёс новые изменения, которые присутствуют в Pull Request из `feature/improve_detection` в `develop`.
Их можно разделить на три группы:

1. Изменения, которые существенно влияют на работу.
   Это в основном добавление новых аугментаций и утилиты для полуконтролируемого обучения.
2. Косметические изменения, облегчающие чтение кода и уменьшающие вероятность ошибок программиста:
   - для некоторых функций добавлены подсказки типов
   - местами проведён рефакторинг (например, внутренним функции в модуле `validate_retinanet` даны названия, начинающиеся с `_`)
3. Тесты.
   Добавлены тесты, в которых запускается обученная нейросеть (в основном, чтобы мне самому было проще понимать, как это происходит).
   Также сделал тест, в котором проверяются различные аугментации.
   Это было удобно для быстрой проверки новых добавленных аугментаций.

### Датасет

Выяснилось, что задача составления набора данных весьма трудоёмкая, и к текущему моменту готово следующее:
- 1 размеченный книжный лист
- 10 размеченных табличек
- 31 неразмеченный лист книжного текста
- 4 неразмеченных рукописных листа
- 62 неразмеченные таблички

Все они [доступны в этом репозитории](https://github.com/braille-systems/brl_ocr/tree/main/data).

Недавно я узнал, что Илья Геннадиевич Оводов использовал при разметке своего датасета AngelinaDataset более слабую нейросеть,
обученную на маленькой выборке, после чего вручную корректировал её ошибки.
Вероятно, следует попробовать что-то такое же.

### Аугментации

Илья Оводов использует библиотеку albumentations, в которой доступны 64 различных трансформации изображения.
Не все они, однако, пригодны для данных с ограничивающими рамками (bounding box), и не все разумно применять.
К примеру, вряд ли имеет смысл применять случайную тень (RandomShadow) или солнечные блики (RandomSunFlare), это будет непохоже на реальные фотографии:

<img width=200 src=https://user-images.githubusercontent.com/23435506/111883324-9ec4dc00-89cb-11eb-9063-eac6e848f16a.jpg> <img width="200" src="https://user-images.githubusercontent.com/23435506/111883370-e6e3fe80-89cb-11eb-84e2-9ed1d24d4480.jpg">

В оригинальной версии AngelinaReader к изображениям применялись небольшие повороты в плоскости картинки, обрезка случайным образом, сглаживание (Blur), изменение яркости и переворот вдоль горизонтальной оси (HorisontalFlip).
Повороты на большой угол делать нельзя, поскольку RetinaNet умеет работать только с прямоугольными BoundingBox'ами, стороны которых параллельны сторонам картинки; если буквы повёрнуты,
прямоугольники начнут накладываться друг на друга. А за наложение даётся большой штраф в целевой функции, чтобы не вышло так:

<img width="300" src="https://user-images.githubusercontent.com/23435506/111883612-80f87680-89cd-11eb-824f-e6e0854bd530.png">

Поэтому ошибки при распознавании повёрнутого текста, видимо, неизбежны.

Я добавил аффинные сдвиги (IAAAffine), перспективные трансформации (IAAPerspective) и изменение масштаба со сдвигом картинки (ShiftScaleRotate).
В результате из иходного изображения с метками

<img width="200" src="https://user-images.githubusercontent.com/23435506/111883742-3fb49680-89ce-11eb-9908-43f08784a031.png">

могут получиться такие:

<img width="200" src="https://user-images.githubusercontent.com/23435506/111883766-65da3680-89ce-11eb-9007-7cb442094755.png">  <img width="200" src="https://user-images.githubusercontent.com/23435506/111883790-8a361300-89ce-11eb-8a5e-8007cc81e832.png">  <img width="200" src="https://user-images.githubusercontent.com/23435506/111883812-a043d380-89ce-11eb-9037-5c463b3710fb.png">

### Полуконтролируемое обучение

Как я узнал уже после того, как приступил к работе, у Ильи Оводова реализовано полуконтролируемое обучение, причём после присвоения картинке псевдометок осуществляется семантическая фильтрация: если псевдометки селадываются в слова, не имеющие смысла, коэффициент в функции потерь для них обнуляется.
Однако я пока что не разобрался в этом до конца и не стал это использовать; мне было интересно всего лишь посмотреть на поведение нейросети при полуконтролируемом обучении с моими "плохими" данными, где есть таблички, искажённые и плохо освещённые листы.

Поэтому была проведена следующая процедура:
1. С нуля обучена (`python model/train.py`) первая нейросеть: тренировочный набор - тренировочный набор DSBI + AngelinaDataset; валидационный набор - валидационная часть DSBI + AngelinaDataset (это настраивается в `model/params.py`).
2. Полученная нейросеть применена к неразмеченной части датасета.
Для этого использован видоизменённый скрипт `run_local.py` (той версии, которая сейчас в ветке `feature/improve_detection`), запущенный следующей командой:
   ```
   python run_local.py brl_ocr\data\unlabeled\plates\ brl_ocr\data\unlabeled\plates\with_pseudolabels --weights_file  weights\run_augment_2021_mar_18\models\best.t7 --param_file weights\run_augment_2021_mar_18\param.txt --save_dev
   python run_local.py brl_ocr\data\unlabeled\plates\ brl_ocr\data\unlabeled\plates\with_pseudolabels --weights_file weights\run_augment_2021_mar_18\models\best.t7 --param_file weights\run_augment_2021_mar_18\param.txt --save_dev
   ```
3. Для полученных картинок с псевдометками с помощью скрипта `data_utils/descriptor_generator.py` сгенерированы файлы-дескрипторы, необходимые, чтобы загрузить картинки при тренировке.
4. В файл `model/params.py` добавлены пути к файлам-дескрипторам в поле `params / data / train_list_filenames`:
   ```python
   train_list_file_names=[
            # < other train datasets >,
            r"brl_ocr/data/unlabeled/plates/with_pseudolabels/train.txt",
            r"brl_ocr/data/unlabeled/golubina/with_pseudolabels/train.txt",
        ],
   ```
5. Запущена вторая итерация тренировки: `python -m model/train.py`

## Результаты

Можно скачать файл `models.zip` (1.9 GB) по адресу [azuev.ddns.net/~valera/angelina_artifacts/](http://azuev.ddns.net/~valera/angelina_artifacts/).
Внутри содержатся три папки с артефактами тренировок:
- без дополнительных аугментаций (`run_noaugment_2021_mar_15`)
- с дополнительными аугментациями (`run_augment_2021_mar_18`)
- с дополнительными аугментациями, обучение с дополнительными данными с псевдо-метками (`run_augment_ssl_1iter_2021_mar_20`)  

В подпапке `output` или `out_examples` в каждой из этих папок - результат запуска на неразмеченной части датасета:
- <...>.labeled.jpg - просто исходная картинка
- <...>.labeled.json - псевдометки в формате labelme
- <...>.marked.jpg - результат распознавания (картинка с подписанными буквами)
- <...>.marked.txt - результат распознавания (текст)

Ниже приведено сравнение характеристик моделей (скрипт `model/validate.py`) на данных, которые включают:
 - тестовую часть DSBI
 - валидационную часть AngelinaDataset - рукописи и книги отдельно
 - 10 размеченных табличек из [brl_ocr](https://github.com/braille-systems/brl_ocr/tree/main/data/labeled/plates)

| модель | набор данных | precision | recall | F1-score |
| --- | --- | --- | --- | --- |
| без аугм | DSBI_test | 0.9921 | 0.9921 | 0.9921 |
| без аугм | Angelina/books | 0.9965 | 0.9851 | 0.9908 |
| без аугм | Angelina/handwritten | 0.9983 | 0.9983 | 0.9983 |
| без аугм | таблички | 0.0 | 0.0 | 0.0 |
| c аугм | DSBI_test | 0.9891 | 0.9840 | 0.9921 |
| c аугм | Angelina/books | 0.9960 | 0.9907 | 0.9933 |
| c аугм | Angelina/handwritten | 0.9965 | 0.9967 | 0.9966 |
| c аугм | таблички | 0.0 | 0.0 | 0.0 |
| аугм + SSL | DSBI_test | 0.9888 | 0.9821 | 0.9854 |
| аугм + SSL | Angelina/books | 0.9962 | 0.9891 | 0.9927 |
| аугм + SSL | Angelina/handwritten | 0.9976 | 0.9983 | 0.998 |
| аугм + SSL | таблички | 0.5882 | 0.08065 | 0.1418 |

Видно, что
- К сожалению, добавление аугментаций практически нигде не привело к улучшению
- При использовании псевдометок и аугментации показатели улучшаются по сравнению с запуском просто с аугментацией, без псевдометок.
Правда, результат на данных DSBI немного ухудшается (возможно, причина в том, что неразмеченные изображения из моего датасета мало похожи на то, что встречается в DSBI).
Важно отметить, что после обучения с псевдо-метками хотя бы часть знаков на табличках распознаётся верно (до того все неправильно).

## Дальнейшие планы

- Конечно же, имеет смысл проверить обучение без дополнительных аугментаций, но с SSL; на это пока что не хватило времени (каждый цикл тренировки на моём компьютере занимает примерно 20 часов).
- Желательно провести каждую тренировку несколько раз и оценить по выборочной дисперсии, насколько статистически значимы полученные различия в качестве ответов.
- Нужно собрать и разметить больше данных
- Нужно разобраться в SSL, реализованом Ильёй Геннадиевичем Оводовым

Также я составил [приблизительный план действий](https://github.com/braille-systems/brl_ocr/wiki/roadmap-march2021-ru), который уже просмотрел Илья Оводов (и выдал мне некоторые рекомендации, которые я ещё буду внимательно изучать).

## Как воспроизвести мои результаты

Значительная часть этого раздела - перевод на русский текста из [README](https://github.com/braille-systems/AngelinaReader/tree/feature/improve_detection#angelina-braille-reader).

### Требования к системе

Вам нужен компьютер с хорошей видеокартой и CUDA. Проверено на Windows 10 с CUDA 10.2.
Кажется, под CUDA11 ещё нет дистрибутива PyTorch.
Помимо того нужны:
- Git + Git LFS (чтобы убедиться, что они есть, наберите в терминале `git lfs --version`)
- Python 3.8 x64

### Установка

1. Выполните команды:
    ```
    git clone --recursive https://github.com/braille-systems/AngelinaReader.git
    cd AngelinaReader
    git clone --recursive https://github.com/braille-systems/brl_ocr.git
    python -m pip install --upgrade pip
    ```
    Здесь вместо `python` может быть что-то другое, что вызывает Python3.8 в Вашей системе, к примеру, `py` или `python3`.
1. Создайте новую виртуальную среду: 
    
    ```
      sudo apt install python3-venv
      python -m venv env
      source env/bin/activate
    ```
    
    (Linux), или же
    
    ```
      python -m pip install virtualenv
      python -m venv env
      .\env\Scripts\activate
    ```
    
    (Windows)
    
    **Замечение:** После каждого рестарта (или в новой сессии терминала) виртуальную среду надо активировать заново (выполнить последнюю команду из блока выше).
    Чтобы убедиться, что Вы успешно активировали среду, исполните команду `which python` (или `where python` в Windows).
    В первой строчке списка Вы должны получить что-то вроде `<...>\AngelinaReader\env\Scripts\python.exe` в Windows (и нечто похожее в других операционных системах).
1. Установите зависимости:
    ```
   python -m pip install --upgrade pip
   python -m pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html
   python -m pip install -r requirements.txt
   python -m pip install -r model/requirements.txt
   ```

### Запуск заранее обученной нейросети

Чтобы убедиться, что всё установлено верно, можно запустить готовую модель:

``` 
wget -O weights/model.t7 http://angelina-reader.ovdv.ru/retina_chars_eced60.clr.008
python run_local.py brl_ocr/data/unlabeled/golubina/IMG_2503.JPG
```

### Тренировка

Тренировка запускается одной командой:
    ```
    python model/train.py
    ```
При появлении ошибки CUDA out of memory можно в `model/params.py` попытаться уменьшить значение `batch_size` или указать
 `device="cpu"` вместо `device="cuda:0"`.

Тренировка займёт какое-то время (~20 часов с видеокартой NVIDIA GeForce GTX 1660 GPU, более суток с 1080Ti).

Результаты работы появляются в папке `NN_results`.
Основные вещи, которые важны для нас в результате - веса модели (`NN_results/<your_run_directory>/models/best.t7`) и параметры
(`NN_results/<your_run_directory>/param.txt`).

Их далее можно использовать для распознавания в скрипте `run_local.py`
(чтобы узнать, какие аргументы нужно передать скрипту, запустите `python run_local.py -h`).

Если Вы хотите воспроизвести полуконтролируемое обучение, следуйте по шагам, изложенным выше в разделе "ход реализации / полуконтролируемое обучение".
   
### Просмотр датасета с метками

Лейблы в `brl_ocr` и `AngelinaDataset` сделаны утилитой [labelme](https://github.com/wkentaro/labelme).
Чтобы их просматривать или изменять, установите одноимённый пакет через pip (можно в той же виртуальной среде, что и AngelinaReader):
```
python -m pip install labelme
labelme --config brl_ocr/.labelmerc
```
Откройте картинку, к примеру, `brl_ocr/data/labeled/books/golubina/IMG_2504.JPG`:

![braille book with rectangles painted around each letter](https://user-images.githubusercontent.com/23435506/111027233-fe394f80-83ff-11eb-9f53-6234c2090dc9.png)
